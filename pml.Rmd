---
title: "Recognizing the qualitiy of execution of weight lifting exercises"
author: "Mario Hofer"
date: "Sunday, July 27, 2014"
output: html_document
---

# Synopsis
This project uses a data set on [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) generated by ambient and on-body sensors to generate a model to determine the quality of execution of weight lifting exercises. A random forest is used to predict the quality of execution. The final model had a strong predictive power, the accuracy for predicting the test set was 99.8%

# Basic approach
The large number of variables and my limitations with regards to the domain-specific knowledge make a theory based approach very difficult. It is hardly possible to determine the correct predictors based on theory. Therefore I chose a rather mechanistic approach to find an adequate set of predictors. My only theory based assumption was that the sensor data from the dumbbel and the forearm should have a large predictive power because the exercise carried out in this data set are dumbbel curls. Therefore the dumbbel itself and the forearm should *"do"* the most work.

The first step consisted of data inspection based on simple summary statistics to get a grasp of the data. Based on this results I continued with pre-processing the data.

```{r, echo=FALSE,comment=NA,message=FALSE,warning=FALSE}
#Initialize environment and load data set
setwd("D:/Sonstiges/Coursera/Data Science/RDir")
library(caret)
library(doParallel)
cl<-makeCluster(detectCores())
registerDoParallel(cl)

dir <- paste(getwd(),"pmlproj",sep="/")
setwd(dir)
train <- read.csv("pml-training.csv",stringsAsFactors=F,na.strings=c("NA",""))
testd <- read.csv("pml-testing.csv",stringsAsFactors=F,na.strings=c("NA",""))
```

# Data inspection and pre-processing
The summary statistics show that most of the variables are of the data type numeric or integer. However one can notice that there are serveral variables with a large number of NAs and furthermore some of the variables have a data type charcter. I started my data cleaning by getting rid of the time-stamp variables, the X variable (which is just a row number) and the user_name variable (because I did not want that my prediction model considers that different users have a significant difference in their quality of execution)

The only non-sensor predictor that I did not drop from the data set was num_window. I was not able to obtain a detailed description on that variable therefore I used a histogram to determine its predictive power. The histogram is given below and since the bars mainly consist of only 1 or 2 colours one can conclude that this variable has a strong predictive power with regards to the quality of execution. Therefore I did not drop it from the data set. 
```{r echo=FALSE}
qplot(num_window,data=train,colour=classe,fill=classe,binwidth=10)
```

The further pre-processing consists of the following rather mechanistic steps:
1. Drop all variables for which 50% or more of the observations feature a missing value. Because with this dataset the NAs heavily depend on the variable and not on the observation one can drastically increase the number of complete cases by only removing some variables (from about 400 to over 19,000).
2. Remove the non-sensor data that should not have any impact on the quality of execution.
3. Drop the variables with near-zero-variation because if there is no variation in the predictor the predictor will not be able to explain any variable in the output.
4. Find sets of variables that are highly correlated and drop those which are less beneficial for the prediction. If two variables are highly correlated one can improve the predictive power of one variable by dropping the other.
5. Check for variables that are linear combinations of other variables. Dropping those will have a similar effect like removing multicollinearity. However the training set did not feature any linear combinations.
6. Splitting the data set into a training and a test set. The training set contains 75% of the observations.

The code for above described pre-processing is given below:
```{r, comment=NA}
# Only keep variables with no more that 50% NAs
df <- train[,colSums(is.na(train))/nrow(train)<0.5]
# Drop non-sensor variables
df <- df[,-c(1:5,ncol(df))]
# Drop variables with very low variation
df <- df[,-nearZeroVar(df)]
# Drop variables that induce multicolinearty problems
df <- df[,-findCorrelation(cor(df),cutoff = 0.75)]
# Drop potential linear combinations of othe variables
remLinComb <- findLinearCombos(df)$remove
if(!is.null(remLinComb)) df <- df[,-remLincomb]
# Add output to clenased dataset
df <- cbind(train$classe,df)
names(df)[1]<-"classe"
set.seed(1337)
inTrain <- createDataPartition(y=df$classe,p=0.75, list=FALSE)
training <- df[inTrain,]
testing <- df[-inTrain,]
```

# Training the model and prediction results
I chose to use a random forest with 25 iterations of bootstrap sampling for the out-of-sample error and the parameter estimation. 

```{r opts_chunk$set(cache=TRUE)}
mf <- train(classe ~ ., method="rf",data=training)
```

## Cross-Validation
I splitted the data into a training set and a test set but I did not carry out any further manual cross-validation because of the properties of the random forest:
*"In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run. Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. [...] This has proven to be unbiased in many tests."* Source: <http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm>

## Training results
The resampling results of the training can be found in the appendix. The optimal model was selected based on the largest accuracy value. 

The results for the final model of the random forest are given below. The OOB estimate of error rate is 0.23%.
```{r, echo=FALSE}
print(mf)
```

# Prediction results
The results for the model applied to the test set are given below. The accuracy is 99.8% with a 95% confidence-interval reaching from 99.63% to 99.9% indicating that the model has a very strong predictive power. 
```{r}
predtest <- predict(mf,newdata=testing)
confusionMatrix(predtest,testing$classe)
```

Taking a look at the computed variable importance confirms the expectations stated during the data inspection:
* num_window has a very strong predictive power
* 6 out of the 10 most important preditors are related to the dumbbell or the forearm
```{r, echo=FALSE}
varImpPlot(mf$finalModel)
```

Finally I applied the model to the 20 test samples required for submisson which yielded the following (correct) results:
```{r, echo=FALSE}
pt <- predict(mf,newdata=testd)
print(as.character(pt))
```      

# References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: <http://groupware.les.inf.puc-rio.br/har#ixzz38g8oik3w>

# Appendix

```{r, echo=FALSE}
print(mf)
```

